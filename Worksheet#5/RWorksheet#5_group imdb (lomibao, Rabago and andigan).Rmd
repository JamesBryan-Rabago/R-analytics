---
title: "RWorksheet#5_group(Lomibao,rabago and andigan)"
output: pdf_document
date: "2024-11-18"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
bow and load the necesssary packages
```{r}
library(kableExtra)
library("rvest")
library("polite")
library("dplyr")
library("stringr")

polite::use_manners(save_as = 'polite_scrape.R')
  
url <- "https://www.imdb.com/chart/toptv/?ref_=nv_tvv_250v"
webpage <- read_html(url)  
 session <- bow(url, 
                 user_agent = "Student education purpose")
 session
 page <- scrape(session)  
```
scraping the title 
```{r}
 title <- webpage%>%html_nodes('h3.ipc-title__text')%>%html_text()
 title <- title[2:26]
 title
```

```{r}
title_list <- as.data.frame(title[1:50])
colnames(title_list)<-"ranks"

```
spliting the data frame
```{r}
split_df <- strsplit(as.character(title_list$ranks),".",fixed = TRUE)
split_df<- data.frame(do.call(rbind,split_df))
split_df
```
renaming the columns
```{r}

split_df<-split_df[-c(3,4)]
colnames(split_df)<- c("Ranks","Titles")
split_df
```
scraping the star- rating and saving in the data frame
```{r}
ratings<- webpage %>% 
  html_nodes('span.ipc-rating-star--rating') %>%
  html_text()
ratings <- as.data.frame(ratings)
```
scraping the numbers of vote 
```{r}
number_votes <- webpage %>%
  html_nodes("span.ipc-rating-star--voteCount") %>%
  html_text()
number_votes <- as.data.frame(number_votes)
```
scraping the number of episode 
```{r}
num_ep <- webpage %>%
  html_nodes('span.sc-300a8231-7.eaXxft.cli-title-metadata-item:nth-of-type(2)')%>%
  html_text()
num_ep
```
cleanig the episode data.
```{r}
episode_counts <- str_extract(num_ep, "\\d+ eps")
number_episode <- str_remove(episode_counts, " eps")
number_episode <- as.data.frame(number_episode)
colnames(number_episode) <- "Episode"
number_episode
```
scraping the year release
```{r}
year <- webpage %>%
  html_nodes('span.sc-300a8231-7.eaXxft.cli-title-metadata-item') %>%html_text()
year
```
Exracting using the regex. 
```{r}
release_years <- str_extract(year, "\\d{4}")
release_years <- release_years[!is.na(release_years)]  # Remove NA values
release_years <- as.numeric(release_years)
release_years <- as.data.frame(release_years)
release_years

```
creating csv file for every one of the data. 
```{r}
#titles
#write.csv(title_list,file = "title.csv")
#rating 
#write.csv(rating,file = "star_rating.csv")
#vote count 
 #write.csv(number_votes,file = "vote_count.csv")
#year
  #write.csv(release_years,file = "year.csv")
#number of episode 
 #write.csv(number_episode,file = "number_episode.csv")
# maam all code is running and correct i just comment it out because of knitting error

```
creating a function for scraping the critic  reviews 
```{r}
get_critic_reviews <- function(link) {
  complete_link <- paste0("https://imdb.com", link)
  show_page <- read_html(complete_link)
  
  # Extract critic reviews
  critic <- show_page %>%
    html_nodes("span.score") %>%  # Adjust this if necessary based on page structure
    html_text()
  
  # Return the second critic review (if available)
  if (length(critic) > 1) {
    return(critic[2])  # Take the second item for the critic score
  } else {
    return(NA)  # If no critic review is found
  }
}
```
 Creating a fuction  to scrape popularity rating for each show
```{r}
get_popularity_rating <- function(link) {
  complete_link <- paste0("https://imdb.com", link)
  show_page <- read_html(complete_link)
  
  # Extract popularity rating
  pop_rating <- show_page %>%
    html_nodes('[data-testid="hero-rating-bar__popularity__score"]') %>%
    html_text()
  
  # Return the popularity rating (if available)
  if (length(pop_rating) > 1) {
    return(pop_rating[2])  # The second item should be the popularity rating
  } else {
    return(NA)  # If no popularity rating is found
  }
}
```
Extracting the links of each shows.
```{r}
links <- webpage %>%
  html_nodes("a.ipc-title-link-wrapper") %>%
  html_attr("href")

```
scrape the links for each critic reviews using the function that u make and using the loop (Sapply)
```{r}
 critic_reviews <- sapply(links, get_critic_reviews)

```
Do the same at the popularity ratings. 
```{r}
popularity_ratings <- sapply(links, get_popularity_rating)
```
Checking if they same length before combining the into a data frame or into a csv file.
```{r}
max_length <- max(length(title_list), length(ratings), length(number_votes), length(number_episode), length(release_years), length(critic_reviews), length(popularity_ratings))
rank_title <- rep(title_list, length.out = max_length)
ratings <- rep(ratings, length.out = max_length)
number_votes <- rep(number_votes, length.out = max_length)
number_episode <- rep(number_episode, length.out = max_length)
release_years <- rep(release_years, length.out = max_length)
critic_reviews <- rep(critic_reviews, length.out = max_length)
popularity_ratings <- rep(popularity_ratings, length.out = max_length)



```
Combining them into a data frame.
```{r}
imdb_top_tv_shows <- data.frame(
  Title = title_list,
  Rating = ratings,
  Votes = number_votes,
  Episode = number_episode,
  Release_Year = release_years,
  ritic_Reviews = critic_reviews,
  Popularity_Rating = popularity_ratings,
  stringsAsFactors = FALSE
)

top_50_shows <- imdb_top_tv_shows %>%
  slice(1:50)  # Get the top 50 shows

print(top_50_shows)
```
Save as csv file.
```{r}
write.csv(top_50_shows, "Top_50_shows.csv")
```
load the necessary packages. 
```{r}
library(rvest)
library(dplyr)
library(stringr)

# Creating a function for IMDB Reviews
imdb_reviews <- function(url) {
  # Load the page content
  page <- tryCatch(read_html(url), error = function(e) NULL)
  if (is.null(page)) {
    message("Failed to load page: ", url)
    return(tibble())
  }

  # Extracting the selected title for the review
  show_title <- page %>%
    html_nodes("h1[data-testid='hero-title-block__title']") %>%
    html_text(trim = TRUE)

  # Checking if the title was extracted
  if (length(show_title) == 0) {
    message("Failed to extract show title for URL: ", url)
    show_title <- NA  # Set to NA if the title is not found
  }

  # Extracting or scraping the review data
  reviewers <- page %>% 
    html_nodes("a.ipc-link.ipc-link--base") %>% 
    html_text() %>% 
    .[. != "Permalink"]

  dates <- page %>% 
    html_nodes("li.ipc-inline-list__item.review-date") %>% 
    html_text()

  rates <- page %>% 
    html_nodes("span.ipc-rating-star--rating") %>% 
    html_text() %>% 
    as.numeric()

  titles <- page %>% 
    html_nodes("h3.ipc-title__text") %>% 
    html_text()

  helpful_votes <- page %>% 
    html_nodes("span.ipc-voting__label__count.ipc-voting__label__count--up") %>% 
    html_text() %>% 
    as.numeric()

  review_texts <- page %>% 
    html_nodes("div.ipc-html-content-inner-div") %>% 
    html_text()

  # Adjust lengths by padding shorter vectors with NA
  max_length <- max(length(reviewers), length(dates), length(rates), length(titles), length(helpful_votes), length(review_texts))

  # Pad vectors with NA if they are shorter than max_length
  reviewers <- c(reviewers, rep(NA, max_length - length(reviewers)))
  dates <- c(dates, rep(NA, max_length - length(dates)))
  rates <- c(rates, rep(NA, max_length - length(rates)))  # Fixed typo here
  titles <- c(titles, rep(NA, max_length - length(titles)))
  helpful_votes <- c(helpful_votes, rep(NA, max_length - length(helpful_votes)))
  review_texts <- c(review_texts, rep(NA, max_length - length(review_texts)))

  # Combine data into a tibble
  tibble(
    show_title = rep(show_title, max_length),  # Add the show title to each review
    reviewer_name = reviewers,
    review_date = dates,
    rating = rates,
    review_title = titles,
    helpful_votes = helpful_votes,
    review_text = review_texts
  )
}

```
list of the links of IMDB for scraping the review.
```{r}
links <- c("https://www.imdb.com/title/tt11126994/reviews/","https://www.imdb.com/title/tt2861424/reviews/","https://www.imdb.com/title/tt1355642/reviews/","https://www.imdb.com/title/tt1355642/reviews/","https://www.imdb.com/title/tt0944947/reviews/")
```
create a empty tibble for storing the data.
```{r}
all_reviews <- tibble()
```
create a loop function in orrder to scrape through links.
```{r}
for (link in links) {
  reviews <- imdb_reviews(link)
  
  # Check if reviews are scraped successfully and limit to 20 reviews per link
  if (nrow(reviews) > 0) {
    reviews <- reviews %>% slice(1:20)  # Limit to the first 20 reviews per link
    all_reviews <- bind_rows(all_reviews, reviews)
  }
}
```
print to check and save in csv file.
```{r}
print(all_reviews)

write.csv(all_reviews, "IMDb_reviews.csv")

```
group for release years and summarise the number of shows.
```{r}

tv_shows_by_year <- imdb_top_tv_shows %>%
  group_by(release_years) %>%
  summarise(Number_of_Shows = n())

```
plotting into a time series.
```{r}


ggplot(tv_shows_by_year, aes(x = release_years, y = Number_of_Shows)) +
  geom_line() +  # Add a line plot
  geom_point() +  # Add points at each data point
  labs(title = "Number of TV Shows Released by Year",
       x = "Year",
       y = "Number of TV Shows Released") +
  theme_gray()
```

